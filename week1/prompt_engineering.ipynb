{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6996af94",
   "metadata": {},
   "source": [
    "# Session 1: Prompt Engineering - with Ollama\n",
    "\n",
    "This notebook is designed for **VS Code** and uses **Ollama** to run local LLM models.\n",
    "\n",
    "**What you’ll do**\n",
    "- Understanding prompt engineering basics\n",
    "- Run a local LLM via Ollama\n",
    "- Learn how to engineer basic prompts to create structured and unstructured outputs\n",
    "- Introduction to RAG in preperation for session 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc827a1c",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac3263",
   "metadata": {},
   "source": [
    "### Setting up Virtual Environment with UV\n",
    "\n",
    "- Install **uv** python package manager using homebrew:\n",
    "    ```bash\n",
    "    brew install uv\n",
    "- Create a virutal environment and download the requirements.txt using:\n",
    "    ```bash\n",
    "    uv venv\n",
    "    uv pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267f455",
   "metadata": {},
   "source": [
    "#### Installing Ollama\n",
    "- Install **Ollama** `https://ollama.com/download` or use:\n",
    "  ```bash\n",
    "  brew install ollama\n",
    "- Start Ollama from applications or by running the following in your terminal:\n",
    "  ```bash\n",
    "  ollama start\n",
    "- Ensure it’s running on `http://localhost:11434`.\n",
    "- Pull one chat model and one embedding model:\n",
    "  ```bash\n",
    "  ollama pull llama3.2\n",
    "  ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa393e7b",
   "metadata": {},
   "source": [
    "## Getting a response from a local model with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce188f",
   "metadata": {},
   "source": [
    "Run the following command in terminal:\n",
    "```bash\n",
    "ollama run llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ee628",
   "metadata": {},
   "source": [
    "Prompt the model with a basic prompt after the trailing arrows e.g.\n",
    "```bash\n",
    ">>> What is the capital of Italy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4aff1",
   "metadata": {},
   "source": [
    "\n",
    "## Generating structured Output (10 minutes?)\n",
    "\n",
    "Now prompt your locally running Ollama model to produce a **strict JSON** object that satisfies the specification below—no prose, no markdown, no trailing commentary.\n",
    "\n",
    "### Specification\n",
    "Produce a single JSON object with the following shape:\n",
    "```json\n",
    "{\n",
    "  \"products\": [\n",
    "    {\n",
    "      \"id\": <integer>,\n",
    "      \"name\": \"<string>\",\n",
    "      \"price\": <float>,\n",
    "      \"tags\": [\"<string>\", \"...\"]\n",
    "    },\n",
    "    \"...\"\n",
    "  ]\n",
    "}\n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721e178",
   "metadata": {},
   "source": [
    "Rules:\n",
    "\n",
    "Include at least 3 products.\n",
    "id must be integer and unique.\n",
    "name is non-empty string.\n",
    "price is a float (not string) and > 0.\n",
    "tags is a non-empty array of strings (no empty strings).\n",
    "Output must be valid JSON with no extra text before/after the JSON block.\n",
    "Do not include comments or explanations.\n",
    "\n",
    "Rubric (10 points total)\n",
    "\n",
    "- Valid JSON (2 pts): Parses without errors; no extra commentary.\n",
    "- Shape compliance (3 pts): Keys exist (products, id, name, price, tags); correct nesting.\n",
    "- Type & content (3 pts): Integer ids (unique), float price, non-empty tags strings.\n",
    "- Quantity (1 pt): At least 3 products.\n",
    "- Cleanliness (1 pt): No additional fields beyond spec (strict mode).\n",
    "\n",
    "\n",
    "Prompting tips:\n",
    "\n",
    "- Use delimiters for the JSON (e.g., “Return only the JSON. Do not include markdown or commentary.”).\n",
    "- Set model behavior (role/tone) and format constraints explicitly.\n",
    "\n",
    "```bash\n",
    "    [Role/Context]: Act as [role] (e.g., AI expert, teacher, marketer).\n",
    "    Your main goal is to [describe task clearly].\n",
    "    Include [specific details, constraints, tone, audience].\n",
    "    Output should be in [format: list, table, paragraph].\n",
    "    [Example/Reference]: (Optional) Here’s an example: [insert example].\n",
    "```\n",
    "- Break down the task into smaller steps if needed\n",
    "- Consider adding few-shot exemplars (mini valid/invalid examples) inside your prompt to steer outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b0f29",
   "metadata": {},
   "source": [
    "### Generating longer pieces of text (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2fb7e",
   "metadata": {},
   "source": [
    "Use the following schema as a guideline to prompt the LLM to generate a longer piece of text. Specify a <b>SYSTEM ROLE</b> and a <b>USER PROMPT</b> and include the specification below in your prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09177187",
   "metadata": {},
   "source": [
    "```json\n",
    "\n",
    "{\n",
    "  \"title\": \"String - working title of the piece\",\n",
    "  \"audience\": \"String - who is this for (e.g., policy makers, junior data scientists)\",\n",
    "  \"purpose\": \"String - inform, persuade, instruct, etc.\",\n",
    "  \"length\": { \"target_words\": 1500, \"tolerance_percent\": 10 },\n",
    "  \"tone_style\": [\"professional\", \"plain-language\", \"neutral\"],\n",
    "  \"format_structure\": [\n",
    "    {\"section\": \"Introduction\", \"requirements\": [\"state problem context\", \"thesis\"]},\n",
    "    {\"section\": \"Background\", \"requirements\": [\"key definitions\", \"prior work\"]},\n",
    "    {\"section\": \"Main Analysis\", \"requirements\": [\"3–5 subheadings\", \"evidence\", \"examples\"]},\n",
    "    {\"section\": \"Risks & Limitations\", \"requirements\": []},\n",
    "    {\"section\": \"Conclusion\", \"requirements\": [\"summary\", \"actionable next steps\"]}\n",
    "  ],\n",
    "  \"constraints\": {\n",
    "    \"citations_required\": true,\n",
    "    \"citation_style\": \"inline links or footnotes\",\n",
    "    \"no_sensitive_data\": true,\n",
    "    \"avoid_jargon\": true\n",
    "  },\n",
    "  \"must_cover\": [\n",
    "    \"Two concrete case studies\",\n",
    "    \"A comparison table for approaches A vs B\",\n",
    "    \"Call-to-action tailored to audience\"\n",
    "  ],\n",
    "  \"banned_content\": [\"marketing claims without evidence\", \"personal data\", \"speculation presented as fact\"],\n",
    "  \"facts_sources\": [\n",
    "    \"Provide 3–5 authoritative sources the model should draw from (titles/links or summaries)\"\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8e52d",
   "metadata": {},
   "source": [
    "## What is Retrieval Augmented Generation (RAG) and why teams use it\n",
    "- The biggest issue with LLMs is hallucination \n",
    "- RAG attempts to solve this by **grounding** the LLM responses in context limited by the user\n",
    "- Documents are chunked, embedded and stored in a vector database such as ChromaDB, Milvus or FAISS\n",
    "- When the system receives a user prompt it conducts a similarity search between the user prompt and the document chunks in the embedding\n",
    "- The top K most similar chunks are **Retrieved**\n",
    "- The original prompt is then **Augmented** with the additional context of the document chunks\n",
    "- The LLM then **Generates** a final response quoting the sources it uses to answer the question\n",
    "\n",
    "**Pipeline**: Index (embed docs → store) → Retrieve (top‑k by similarity) → Generate (LLM with prompt + context).\n",
    "\n",
    "There are various frameworks we can use to build RAG pipelines such as Langchain, Haystack and LLamaIndex. In the next session we will be exploring how to do this with **Haystack**\n",
    "\n",
    "![Rag Pipeline](https://admin.bentoml.com/uploads/medium_simple_rag_workflow_091648ef39.png )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
