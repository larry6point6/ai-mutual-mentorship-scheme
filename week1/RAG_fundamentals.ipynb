{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6996af94",
   "metadata": {},
   "source": [
    "# RAG Fundamentals - with Ollama\n",
    "\n",
    "This notebook is designed for **VS Code** and uses **Haystack 2.x** with **Ollama** for local LLM + embeddings, and **Chroma** as the vector store.\n",
    "\n",
    "**What you’ll do**\n",
    "- Understand RAG and prompt engineering basics\n",
    "- Run a local LLM via Ollama from Haystack\n",
    "- Index small docs into Chroma and query them with embeddings\n",
    "- Build a minimal RAG pipeline using `PromptBuilder` + `OllamaGenerator`\n",
    "- Save your task‑specific prompt for next week’s LLM‑as‑judge evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc827a1c",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac3263",
   "metadata": {},
   "source": [
    "### Setting up Virtual Environment with UV\n",
    "\n",
    "- Install **uv** python package manager using homebrew:\n",
    "    ```bash\n",
    "    brew install uv\n",
    "- Create a virutal environment and download the requirements.txt using:\n",
    "    ```bash\n",
    "    uv venv\n",
    "    uv pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267f455",
   "metadata": {},
   "source": [
    "#### Installing Ollama\n",
    "- Install **Ollama** `https://ollama.com/download` or use:\n",
    "  ```bash\n",
    "  brew install ollama\n",
    "- Start Ollama from applications or by running the following in your terminal:\n",
    "  ```bash\n",
    "  ollama start\n",
    "- Ensure it’s running on `http://localhost:11434`.\n",
    "- Pull one chat model and one embedding model:\n",
    "  ```bash\n",
    "  ollama pull llama3.2\n",
    "  ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0274af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OLLAMA_ENDPOINT': 'http://localhost:11434', 'OLLAMA_MODEL': 'llama3.2', 'EMBED_MODEL': 'nomic-embed-text'}\n"
     ]
    }
   ],
   "source": [
    "# Configure models via environment variables for easy swapping\n",
    "import os\n",
    "os.environ.setdefault(\"OLLAMA_ENDPOINT\", \"http://localhost:11434\")\n",
    "os.environ.setdefault(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "os.environ.setdefault(\"EMBED_MODEL\", \"nomic-embed-text\")\n",
    "print({k: os.environ[k] for k in [\"OLLAMA_ENDPOINT\",\"OLLAMA_MODEL\",\"EMBED_MODEL\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faaiz.shanawas/Documents/week1/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama reachable: 200 models: ['nomic-embed-text:latest', 'qwen2.5:14b']\n"
     ]
    }
   ],
   "source": [
    "# Quick reachability check to Ollama\n",
    "import requests, os\n",
    "url = os.environ[\"OLLAMA_ENDPOINT\"].rstrip('/') + \"/api/tags\"\n",
    "r = requests.get(url, timeout=10)\n",
    "print(\"Ollama reachable:\", r.status_code, \"models:\", [m.get('name') for m in r.json().get('models',[])][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8e52d",
   "metadata": {},
   "source": [
    "## What is RAG and why teams use it\n",
    "RAG retrieves context from a vector store, and the LLM answers **grounded** in that context—improving accuracy, transparency, and domain fit.\n",
    "\n",
    "**Pipeline**: Index (embed docs → store) → Retrieve (top‑k by similarity) → Generate (LLM with prompt + context).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef286725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haystack OK 2.21.0\n",
      "haystack_integrations OK \n",
      "chroma_haystack missing → pip install -r requirements.txt No module named 'chroma_haystack'\n",
      "ollama OK \n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "for pkg in [\"haystack\",\"haystack_integrations\",\"chroma_haystack\",\"ollama\"]:\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        print(pkg, \"OK\", getattr(m, \"__version__\", \"\"))\n",
    "    except Exception as e:\n",
    "        print(pkg, \"missing → pip install -r requirements.txt\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa393e7b",
   "metadata": {},
   "source": [
    "## Getting a response from a local model with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce188f",
   "metadata": {},
   "source": [
    "Run the following command in terminal:\n",
    "```bash\n",
    "ollama run llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ee628",
   "metadata": {},
   "source": [
    "Prompt the model with a basic prompt after the trailing arrows e.g.\n",
    "```bash\n",
    ">>> What is the capital of Italy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4aff1",
   "metadata": {},
   "source": [
    "\n",
    "## Generating structured Output\n",
    "\n",
    "Now prompt your locally running Ollama model to produce a **strict JSON** object that satisfies the specification below—no prose, no markdown, no trailing commentary.\n",
    "\n",
    "### Specification\n",
    "Produce a single JSON object with the following shape:\n",
    "```json\n",
    "{\n",
    "  \"products\": [\n",
    "    {\n",
    "      \"id\": <integer>,\n",
    "      \"name\": \"<string>\",\n",
    "      \"price\": <float>,\n",
    "      \"tags\": [\"<string>\", \"...\"]\n",
    "    },\n",
    "    \"...\"\n",
    "  ]\n",
    "}\n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721e178",
   "metadata": {},
   "source": [
    "Rules:\n",
    "\n",
    "Include at least 3 products.\n",
    "id must be integer and unique.\n",
    "name is non-empty string.\n",
    "price is a float (not string) and > 0.\n",
    "tags is a non-empty array of strings (no empty strings).\n",
    "Output must be valid JSON with no extra text before/after the JSON block.\n",
    "Do not include comments or explanations.\n",
    "\n",
    "Rubric (10 points total)\n",
    "\n",
    "- Valid JSON (2 pts): Parses without errors; no extra commentary.\n",
    "- Shape compliance (3 pts): Keys exist (products, id, name, price, tags); correct nesting.\n",
    "- Type & content (3 pts): Integer ids (unique), float price, non-empty tags strings.\n",
    "- Quantity (1 pt): At least 3 products.\n",
    "- Cleanliness (1 pt): No additional fields beyond spec (strict mode).\n",
    "\n",
    "\n",
    "Prompting tips:\n",
    "\n",
    "Use delimiters for the JSON (e.g., “Return only the JSON. Do not include markdown or commentary.”).\n",
    "Set model behavior (role/tone) and format constraints explicitly.\n",
    "Consider adding few-shot exemplars (mini valid/invalid examples) inside your prompt to steer outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020d145",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3b0f29",
   "metadata": {},
   "source": [
    "### Generating longer pieces of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2fb7e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
