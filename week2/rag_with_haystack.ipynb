{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: RAG basics with Haystack and Opensearch\n",
    "\n",
    "This notebook is designed for **VS Code** and uses **Ollama** to run local LLM models.\n",
    "\n",
    "**What you’ll do**\n",
    "- Recap the concept of RAG\n",
    "- Use the Haystack library to explore RAG components\n",
    "- Build a vector store using open source text documents\n",
    "- Build a RAG pipeline with an in-memory document store\n",
    "- Explore Opensearch and Hybrid retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2c863",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Ollama running locally (`http://localhost:11434`) with a chat model (e.g., `llama3.2`) and an embedding model (e.g., `nomic-embed-text`).\n",
    "- Recommended Python 3.11\n",
    "- Install requirements.txt\n",
    "\n",
    "```bash\n",
    "uv python install 3.11\n",
    "uv venv --python 3.11\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7820b",
   "metadata": {},
   "source": [
    "### Import all the packages we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever, InMemoryBM25Retriever\n",
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder, OllamaTextEmbedder\n",
    "\n",
    "print(\"Haystack & integrations imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — LLM‑as‑Judge (evaluate last week's outputs)\n",
    "Paste any text you generated last week and score it against a rubric using the local model. This illustrates *prompt‑as‑program* patterns and **structured outputs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6546792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>, _content=[TextContent(text='Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. It enables computers to understand, interpret, and generate human language, such as text or speech.\\n\\nNLP involves a range of techniques and algorithms that allow computers to:\\n\\n1. **Text Analysis**: Extract insights from unstructured text data, like sentiment analysis (e.g., determining whether a piece of text is positive or negative) or named entity recognition (identifying specific entities like names or locations).\\n2. **Language Understanding**: Interpret the meaning of language, including syntax, semantics, and pragmatics.\\n3. **Machine Translation**: Translate text from one language to another.\\n4. **Speech Recognition**: Convert spoken words into written text.\\n5. **Chatbots and Virtual Assistants**: Develop conversational interfaces that can understand and respond to user queries.\\n\\nNLP has many applications in areas like:\\n\\n1. **Customer Service**: Automating chatbots to provide customer support.\\n2. **Sentiment Analysis**: Analyzing social media posts to gauge public opinion.\\n3. **Language Learning**: Developing tools to help language learners practice their skills.\\n4. **Information Retrieval**: Building search engines that can accurately understand user queries.\\n\\nThe goal of NLP is to create machines that can communicate effectively with humans, making interactions more efficient and effective.\\n\\nWould you like to know more about a specific aspect of NLP?')], _name=None, _meta={'model': 'llama3.2', 'done': True, 'total_duration': 7955921792, 'load_duration': 141746042, 'prompt_eval_duration': 326693666, 'eval_duration': 5372706925, 'logprobs': None, 'finish_reason': 'stop', 'completion_start_time': '2026-01-29T11:48:39.622611Z', 'usage': {'completion_tokens': 293, 'prompt_tokens': 92, 'total_tokens': 385}})]}\n"
     ]
    }
   ],
   "source": [
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "generator = OllamaChatGenerator(model=\"llama3.2\",\n",
    "                            url = \"http://localhost:11434\",\n",
    "                            generation_kwargs={\n",
    "                              \"temperature\": 0.7,\n",
    "                              })\n",
    "\n",
    "system_message = \"You are a helpful, respectful and honest assistant. Always answer as \\\n",
    "    helpfully as possible, while being safe. Your answers should not include any harmful,\\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your/ \\\n",
    "    responses are socially unbiased and positive in nature.\"\n",
    "user_message = \"What's Natural Language Processing?\"\n",
    "messages = [ChatMessage.from_system(system_message),\n",
    "ChatMessage.from_user(user_message)]\n",
    "\n",
    "output = generator.run(messages=messages)\n",
    "print(output)\n",
    "\n",
    "## Can you make the output print nicely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Build a Small Vector Database (Public Domain)\n",
    "We’ll load short excerpts from public‑domain texts (Project Gutenberg) to keep the demo fast. You can replace, extend, or ingest your own local files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alice': 20000, 'pride': 20000, 'artofwar': 20000}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "# you can swap out these URLs with any public text URLs you like\n",
    "PUBLIC_URLS = {\n",
    "    \"alice\": \"https://www.gutenberg.org/ebooks/11.txt.utf-8\",\n",
    "    \"pride\": \"https://www.gutenberg.org/ebooks/1342.txt.utf-8\",\n",
    "    \"artofwar\": \"https://www.gutenberg.org/files/17405/17405-h/17405-h.htm\",\n",
    "}\n",
    "\n",
    "RAW_DOCS = {}\n",
    "for k,u in PUBLIC_URLS.items():\n",
    "    try:\n",
    "        # fetch the text from the url\n",
    "        t = requests.get(u, timeout=20).text\n",
    "        RAW_DOCS[k] = t[:20000]  # first 20k chars per title for speed\n",
    "    except Exception as e:\n",
    "        print(\"Fetch failed for\", k, e)\n",
    "        RAW_DOCS[k] = \"\"\n",
    "\n",
    "print({k: len(v) for k,v in RAW_DOCS.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(id=338b7a2313cf2dc652f635b7b78377cc7882cb441f3a53f00f76496be1f313c6, content: '﻿The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "    \n",
      "This ebook is for the use of...', meta: {'source': 'alice'})\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document\n",
    "# Create a list of Haystack Documents \n",
    "DOCS=[]\n",
    "for name, text in RAW_DOCS.items():\n",
    "    if not text: continue\n",
    "    DOCS.append(Document(content=text, meta={\"source\": name}))\n",
    "\n",
    "print(DOCS[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 — Component Demos (In‑Memory Store)\n",
    "We’ll demonstrate **Ollama embeddings** + **InMemoryEmbeddingRetriever** (dense), and **InMemoryBM25Retriever** (sparse). Then we combine them with a **DocumentJoiner** and optional **ranker** for a simple hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "splitter = DocumentSplitter(split_by=\"word\", split_length=300, split_overlap=50)\n",
    "chunked_docs = splitter.run(documents=DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder\n",
    "\n",
    "# Embed documents with Ollama and write\n",
    "embedder = OllamaDocumentEmbedder(model=\"nomic-embed-text\", url=\"http://localhost:11434\")\n",
    "\n",
    "# Pass the list of haystack documents and run the embedder\n",
    "embedded_docs = embedder.run(chunked_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fa5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "# Initialize In-Memory Document Store (Vector Database)\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# Write the embedded documents to the document store\n",
    "document_store.write_documents([embedded_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e429be",
   "metadata": {},
   "source": [
    "### Retrieval (Dense + Sparse Retrieval)\n",
    "https://haystack.deepset.ai/blog/hybrid-retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two retrievers\n",
    "emb_retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=5) # dense retreiver\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=document_store, top_k=5) # sparse retriever\n",
    "\n",
    "# Query examples\n",
    "QUERIES = [\n",
    "    \"What is Retrieval-Augmented Generation?\",\n",
    "    \"Who is Alice and what happens at the start of the story?\",\n",
    "    \"What does Sun Tzu say about deception?\",\n",
    "]\n",
    "\n",
    "for q in QUERIES:\n",
    "    print(\"\\nQuery:\", q)\n",
    "    r1 = emb_retriever.run(query_embedding=OllamaTextEmbedder(model=\"nomic-embed-text\", url=\"http://localhost:11434\").run(text=q)[\"embedding\"])  # dense\n",
    "    r2 = bm25_retriever.run(query=q)  # sparse\n",
    "    print(\"Dense top sources:\", [d.meta[\"source\"] for d in r1[\"documents\"]])\n",
    "    print(\"Sparse top sources:\", [d.meta[\"source\"] for d in r2[\"documents\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530a4a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple hybrid: join results (RRF‑style ranker optional)\n",
    "joiner = DocumentJoiner(join_mode=\"reciprocal_rank_fusion\")\n",
    "ranker = TransformersSimilarityRanker(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "hybrid = Pipeline()\n",
    "hybrid.add_component(\"q_embedder\", OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT))\n",
    "hybrid.add_component(\"dense\", emb_retriever)\n",
    "hybrid.add_component(\"sparse\", bm25_retriever)\n",
    "hybrid.add_component(\"join\", joiner)\n",
    "hybrid.add_component(\"rerank\", ranker)\n",
    "\n",
    "hybrid.connect(\"q_embedder.embedding\", \"dense.query_embedding\")\n",
    "hybrid.connect(\"sparse\", \"join.documents\")\n",
    "hybrid.connect(\"dense\", \"join.documents\")\n",
    "hybrid.connect(\"join.documents\", \"rerank.documents\")\n",
    "\n",
    "for q in QUERIES:\n",
    "    out = hybrid.run({\"q_embedder\": {\"text\": q}})\n",
    "    print(\"\\nHybrid top sources for:\", q)\n",
    "    print([d.meta[\"source\"] for d in out[\"rerank\"][\"documents\"][:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 — RAG Pipeline (In‑Memory)\n",
    "We wire **retrieval → prompt construction → generation** using **PromptBuilder** and **OllamaGenerator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_tmpl = \"\"\"\n",
    "You are a precise assistant. Answer using ONLY the provided context.\n",
    "Cite the source names in brackets.\n",
    "\n",
    "Context:\n",
    "{% for d in documents %}- [{{ d.meta.source }}] {{ d.content[:300] }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer in 3-5 sentences.\n",
    "\"\"\"\n",
    "\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"q_embedder\", OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT))\n",
    "rag.add_component(\"dense\", InMemoryEmbeddingRetriever(document_store=store, top_k=5))\n",
    "rag.add_component(\"prompt\", PromptBuilder(template=prompt_tmpl))\n",
    "rag.add_component(\"llm\", OllamaGenerator(model=OLLAMA_MODEL, url=OLLAMA_ENDPOINT))\n",
    "\n",
    "rag.connect(\"q_embedder.embedding\", \"dense.query_embedding\")\n",
    "rag.connect(\"dense.documents\", \"prompt.documents\")\n",
    "rag.connect(\"prompt\", \"llm\")\n",
    "\n",
    "answer = rag.run({\"q_embedder\": {\"text\": \"Summarise Sun Tzu's stance on deception.\"},\n",
    "                  \"prompt\": {\"query\": \"Summarise Sun Tzu's stance on deception.\"}})\n",
    "print(answer[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 — Vector DBs & OpenSearch (Docker) — Hybrid Retrieval\n",
    "We’ll now use **OpenSearch** as the document store, then compare **BM25**, **dense embeddings**, and the **OpenSearchHybridRetriever**.\n",
    "\n",
    "> Quickstart (local dev):\n",
    "```bash\n",
    "# Single node, security disabled for local testing (see official docs for options)\n",
    "docker run -p 9200:9200 -p 9600:9600   -e \"discovery.type=single-node\"   -e \"DISABLE_SECURITY_PLUGIN=true\"   --name opensearch   -d opensearchproject/opensearch:latest\n",
    "```\n",
    "OpenSearch Haystack integration: `pip install opensearch-haystack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack_integrations.components.retrievers.opensearch import (\n",
    "    OpenSearchBM25Retriever,\n",
    "    OpenSearchEmbeddingRetriever,\n",
    "    OpenSearchHybridRetriever,\n",
    ")\n",
    "\n",
    "# Adjust embedding_dim to your embedding model; nomic-embed-text -> 768\n",
    "OPENSEARCH = {\n",
    "    \"hosts\": [\"http://localhost:9200\"],\n",
    "    \"index\": \"demo_docs\",\n",
    "    \"embedding_dim\": 768,\n",
    "}\n",
    "\n",
    "doc_store = OpenSearchDocumentStore(**OPENSEARCH)\n",
    "\n",
    "# Embed with Ollama and write\n",
    "op_embedder = OllamaDocumentEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT)\n",
    "docs_emb = op_embedder.run(DOCS)\n",
    "doc_store.write_documents(docs_emb[\"documents\"])  # index\n",
    "print(\"OpenSearch indexed docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Three retrievers\n",
    "os_bm25 = OpenSearchBM25Retriever(document_store=doc_store, top_k=5)\n",
    "os_emb  = OpenSearchEmbeddingRetriever(document_store=doc_store, top_k=5)\n",
    "# Hybrid retriever combines both under the hood\n",
    "os_hybrid = OpenSearchHybridRetriever(document_store=doc_store,\n",
    "                                     embedder=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT),\n",
    "                                     top_k=5)\n",
    "\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "print(\"BM25:\")\n",
    "print([d.meta.get(\"source\") for d in os_bm25.run(query=query)[\"documents\"]])\n",
    "print(\"Embedding:\")\n",
    "print([d.meta.get(\"source\") for d in os_emb.run(query_embedding=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT).run(text=query)[\"embedding\"])][\"documents\"])  # noqa\n",
    "print(\"Hybrid:\")\n",
    "print([d.meta.get(\"source\") for d in os_hybrid.run(query=query)[\"documents\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
