{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: RAG basics with Haystack and Opensearch\n",
    "\n",
    "This notebook is designed for **VS Code** and uses **Ollama** to run local LLM models.\n",
    "\n",
    "**What you’ll do**\n",
    "- Recap the concept of RAG\n",
    "- Use the Haystack library to explore RAG components\n",
    "- Build a vector store using open source text documents\n",
    "- Build a RAG pipeline with an in-memory document store\n",
    "- Explore Opensearch and Hybrid retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2c863",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Ollama running locally (`http://localhost:11434`) with a chat model (e.g., `llama3.2`) and an embedding model (e.g., `nomic-embed-text`).\n",
    "- Recommended Python 3.11\n",
    "- Install requirements.txt\n",
    "\n",
    "```bash\n",
    "uv python3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01279cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Prereqs**\n",
    "- Ollama running locally (`http://localhost:11434`) with a chat model (e.g., `llama3.2`) and an embedding model (e.g., `nomic-embed-text`).\n",
    "- Recommended Python 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, requests\n",
    "OLLAMA_ENDPOINT=os.getenv(\"OLLAMA_ENDPOINT\",\"http://localhost:11434\").rstrip('/')\n",
    "OLLAMA_MODEL=os.getenv(\"OLLAMA_MODEL\",\"llama3.2\")\n",
    "EMBED_MODEL=os.getenv(\"EMBED_MODEL\",\"nomic-embed-text\")\n",
    "print({\"OLLAMA_ENDPOINT\":OLLAMA_ENDPOINT,\"OLLAMA_MODEL\":OLLAMA_MODEL,\"EMBED_MODEL\":EMBED_MODEL})\n",
    "\n",
    "# Ping Ollama\n",
    "try:\n",
    "    r=requests.get(OLLAMA_ENDPOINT+\"/api/tags\", timeout=10)\n",
    "    print(\"Ollama reachable:\", r.status_code, [m.get('name') for m in r.json().get('models',[])][:5])\n",
    "except Exception as e:\n",
    "    print(\"Warning: Ollama not reachable:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever, InMemoryBM25Retriever\n",
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder, OllamaTextEmbedder\n",
    "\n",
    "print(\"Haystack & integrations imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A — LLM‑as‑Judge (evaluate last week's outputs)\n",
    "Paste any text you generated last week and score it against a rubric using the local model. This illustrates *prompt‑as‑program* patterns and **structured outputs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a rubric and ask the model to return STRICT JSON only\n",
    "rubric = {\n",
    "  \"length\": \"~300 words\",\n",
    "  \"tone\": \"professional, informative\",\n",
    "  \"structure\": [\"intro\", \"2-3 body paragraphs\", \"conclusion\"],\n",
    "  \"must_include\": [\"definition of RAG\", \"one concrete example\"],\n",
    "  \"must_avoid\": [\"jargon without explanation\", \"undefined acronyms\"]\n",
    "}\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "You are a meticulous evaluator. Score the SUBMISSION against the RUBRIC.\n",
    "\n",
    "Rules:\n",
    "- Return ONLY valid JSON (no commentary, no markdown)\n",
    "- Fields: {\"score_total\": int (0-10), \"feedback\": [\"...\"], \"violations\": [\"...\"], \"passes\": [\"...\"]}\n",
    "\n",
    "RUBRIC = {rubric}\n",
    "SUBMISSION = \"\"\"\n",
    "\n",
    "submission = \"Paste prior text here and run this cell.\"\n",
    "\n",
    "llm = OllamaGenerator(model=OLLAMA_MODEL, url=OLLAMA_ENDPOINT)\n",
    "judge_prompt = JUDGE_PROMPT.replace('{rubric}', str(rubric)) + submission\n",
    "out = llm.run(prompt=judge_prompt)\n",
    "print(out[\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B — Build a Small Corpus (Public Domain)\n",
    "We’ll load short excerpts from public‑domain texts (Project Gutenberg) to keep the demo fast. You can replace, extend, or ingest your own local files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests, textwrap\n",
    "\n",
    "PUBLIC_URLS = {\n",
    "    \"alice\": \"https://www.gutenberg.org/ebooks/11.txt.utf-8\",\n",
    "    \"pride\": \"https://www.gutenberg.org/ebooks/1342.txt.utf-8\",\n",
    "    \"artofwar\": \"https://www.gutenberg.org/files/17405/17405-h/17405-h.htm\",\n",
    "}\n",
    "\n",
    "RAW_DOCS = {}\n",
    "for k,u in PUBLIC_URLS.items():\n",
    "    try:\n",
    "        t = requests.get(u, timeout=20).text\n",
    "        RAW_DOCS[k] = t[:20000]  # first 20k chars per title for speed\n",
    "    except Exception as e:\n",
    "        print(\"Fetch failed for\", k, e)\n",
    "        RAW_DOCS[k] = \"\"\n",
    "\n",
    "print({k: len(v) for k,v in RAW_DOCS.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Haystack Documents (small chunks)\n",
    "from textwrap import wrap\n",
    "\n",
    "DOCS=[]\n",
    "for name, text in RAW_DOCS.items():\n",
    "    if not text: continue\n",
    "    chunks = wrap(text, 1200)  # ~1200 char chunks\n",
    "    for i, ch in enumerate(chunks[:30]):  # cap per title\n",
    "        DOCS.append(Document(content=ch, meta={\"source\": name, \"chunk\": i}))\n",
    "\n",
    "len(DOCS), DOCS[0].meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C — Component Demos (In‑Memory Store)\n",
    "We’ll demonstrate **Ollama embeddings** + **InMemoryEmbeddingRetriever** (dense), and **InMemoryBM25Retriever** (sparse). Then we combine them with a **DocumentJoiner** and optional **ranker** for a simple hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store = InMemoryDocumentStore()\n",
    "writer = DocumentWriter(store)\n",
    "\n",
    "# Embed documents with Ollama and write\n",
    "embedder = OllamaDocumentEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT)\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)\n",
    "indexing.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing.run({\"embedder\": {\"documents\": DOCS}})\n",
    "print(\"Docs in store:\", store.count_documents())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two retrievers\n",
    "emb_retriever = InMemoryEmbeddingRetriever(document_store=store, top_k=5)\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=store, top_k=5)\n",
    "\n",
    "# Query examples\n",
    "QUERIES = [\n",
    "    \"What is Retrieval-Augmented Generation?\",\n",
    "    \"Who is Alice and what happens at the start of the story?\",\n",
    "    \"What does Sun Tzu say about deception?\",\n",
    "]\n",
    "\n",
    "for q in QUERIES:\n",
    "    print(\"\n",
    "=== Query:\", q)\n",
    "    r1 = emb_retriever.run(query_embedding=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT).run(text=q)[\"embedding\"])  # dense\n",
    "    r2 = bm25_retriever.run(query=q)  # sparse\n",
    "    print(\"Dense top sources:\", [d.meta[\"source\"] for d in r1[\"documents\"]])\n",
    "    print(\"Sparse top sources:\", [d.meta[\"source\"] for d in r2[\"documents\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple hybrid: join results (RRF‑style ranker optional)\n",
    "joiner = DocumentJoiner(join_mode=\"reciprocal_rank_fusion\")\n",
    "ranker = TransformersSimilarityRanker(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "hybrid = Pipeline()\n",
    "hybrid.add_component(\"q_embedder\", OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT))\n",
    "hybrid.add_component(\"dense\", emb_retriever)\n",
    "hybrid.add_component(\"sparse\", bm25_retriever)\n",
    "hybrid.add_component(\"join\", joiner)\n",
    "hybrid.add_component(\"rerank\", ranker)\n",
    "\n",
    "hybrid.connect(\"q_embedder.embedding\", \"dense.query_embedding\")\n",
    "hybrid.connect(\"sparse\", \"join.documents\")\n",
    "hybrid.connect(\"dense\", \"join.documents\")\n",
    "hybrid.connect(\"join.documents\", \"rerank.documents\")\n",
    "\n",
    "for q in QUERIES:\n",
    "    out = hybrid.run({\"q_embedder\": {\"text\": q}})\n",
    "    print(\"\n",
    "=== Hybrid top sources for:\", q)\n",
    "    print([d.meta[\"source\"] for d in out[\"rerank\"][\"documents\"][:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D — RAG Pipeline (In‑Memory)\n",
    "We wire **retrieval → prompt construction → generation** using **PromptBuilder** and **OllamaGenerator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_tmpl = \"\"\"\n",
    "You are a precise assistant. Answer using ONLY the provided context.\n",
    "Cite the source names in brackets.\n",
    "\n",
    "Context:\n",
    "{% for d in documents %}- [{{ d.meta.source }}] {{ d.content[:300] }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer in 3-5 sentences.\n",
    "\"\"\"\n",
    "\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"q_embedder\", OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT))\n",
    "rag.add_component(\"dense\", InMemoryEmbeddingRetriever(document_store=store, top_k=5))\n",
    "rag.add_component(\"prompt\", PromptBuilder(template=prompt_tmpl))\n",
    "rag.add_component(\"llm\", OllamaGenerator(model=OLLAMA_MODEL, url=OLLAMA_ENDPOINT))\n",
    "\n",
    "rag.connect(\"q_embedder.embedding\", \"dense.query_embedding\")\n",
    "rag.connect(\"dense.documents\", \"prompt.documents\")\n",
    "rag.connect(\"prompt\", \"llm\")\n",
    "\n",
    "answer = rag.run({\"q_embedder\": {\"text\": \"Summarise Sun Tzu's stance on deception.\"},\n",
    "                  \"prompt\": {\"query\": \"Summarise Sun Tzu's stance on deception.\"}})\n",
    "print(answer[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E — Vector DBs & OpenSearch (Docker) — Hybrid Retrieval\n",
    "We’ll now use **OpenSearch** as the document store, then compare **BM25**, **dense embeddings**, and the **OpenSearchHybridRetriever**.\n",
    "\n",
    "> Quickstart (local dev):\n",
    "```bash\n",
    "# Single node, security disabled for local testing (see official docs for options)\n",
    "docker run -p 9200:9200 -p 9600:9600   -e \"discovery.type=single-node\"   -e \"DISABLE_SECURITY_PLUGIN=true\"   --name opensearch   -d opensearchproject/opensearch:latest\n",
    "```\n",
    "OpenSearch Haystack integration: `pip install opensearch-haystack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack_integrations.components.retrievers.opensearch import (\n",
    "    OpenSearchBM25Retriever,\n",
    "    OpenSearchEmbeddingRetriever,\n",
    "    OpenSearchHybridRetriever,\n",
    ")\n",
    "\n",
    "# Adjust embedding_dim to your embedding model; nomic-embed-text -> 768\n",
    "OPENSEARCH = {\n",
    "    \"hosts\": [\"http://localhost:9200\"],\n",
    "    \"index\": \"demo_docs\",\n",
    "    \"embedding_dim\": 768,\n",
    "}\n",
    "\n",
    "doc_store = OpenSearchDocumentStore(**OPENSEARCH)\n",
    "\n",
    "# Embed with Ollama and write\n",
    "op_embedder = OllamaDocumentEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT)\n",
    "docs_emb = op_embedder.run(DOCS)\n",
    "doc_store.write_documents(docs_emb[\"documents\"])  # index\n",
    "print(\"OpenSearch indexed docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Three retrievers\n",
    "os_bm25 = OpenSearchBM25Retriever(document_store=doc_store, top_k=5)\n",
    "os_emb  = OpenSearchEmbeddingRetriever(document_store=doc_store, top_k=5)\n",
    "# Hybrid retriever combines both under the hood\n",
    "os_hybrid = OpenSearchHybridRetriever(document_store=doc_store,\n",
    "                                     embedder=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT),\n",
    "                                     top_k=5)\n",
    "\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "print(\"\n",
    "BM25:\")\n",
    "print([d.meta.get(\"source\") for d in os_bm25.run(query=query)[\"documents\"]])\n",
    "print(\"\n",
    "Embedding:\")\n",
    "print([d.meta.get(\"source\") for d in os_emb.run(query_embedding=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT).run(text=query)[\"embedding\"])][\"documents\"])  # noqa\n",
    "print(\"\n",
    "Hybrid:\")\n",
    "print([d.meta.get(\"source\") for d in os_hybrid.run(query=query)[\"documents\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Retrieval Quality with an LLM‑as‑Judge\n",
    "We’ll ask the model to pick which result set (BM25 vs Dense vs Hybrid) best matches the query based on top‑k snippets. This is a quick, *didactic* judge; for production you’d also compute offline metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import islice\n",
    "\n",
    "def top_snippets(docs, n=3):\n",
    "    return [f\"[{d.meta.get('source')}] {d.content[:220].replace('\n",
    "',' ')}\" for d in islice(docs, n)]\n",
    "\n",
    "COMPARE_PROMPT = \"\"\"\n",
    "You are a fair, rigorous judge. Given a QUERY and three candidate result sets, choose which set is MOST relevant.\n",
    "Return ONLY JSON: {\"winner\": \"bm25|dense|hybrid\", \"rationale\": \"...\"}\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "BM25_TOP: {bm25}\n",
    "DENSE_TOP: {dense}\n",
    "HYBRID_TOP: {hybrid}\n",
    "\"\"\"\n",
    "\n",
    "q = \"Who is Alice and what happens at the start of the story?\"\n",
    "res_bm25 = os_bm25.run(query=q)[\"documents\"]\n",
    "res_dense = os_emb.run(query_embedding=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT).run(text=q)[\"embedding\"]) [\"documents\"]\n",
    "res_hybrid= os_hybrid.run(query=q)[\"documents\"]\n",
    "\n",
    "prompt = COMPARE_PROMPT.format(query=q,\n",
    "                               bm25=\"\n",
    "\".join(top_snippets(res_bm25)),\n",
    "                               dense=\"\n",
    "\".join(top_snippets(res_dense)),\n",
    "                               hybrid=\"\n",
    "\".join(top_snippets(res_hybrid)))\n",
    "judge = OllamaGenerator(model=OLLAMA_MODEL, url=OLLAMA_ENDPOINT)\n",
    "print(judge.run(prompt)[\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Learned\n",
    "- How to use Haystack components individually, then compose into a RAG pipeline.\n",
    "- Why vector databases matter and how OpenSearch supports BM25, vector search, and **hybrid** retrieval in Haystack.\n",
    "- How an **LLM‑as‑judge** can help you compare retrieval strategies for teaching and iteration.\n",
    "\n",
    "**Next:** Persist vector stores, add chunking/cleaning pipelines, and evaluate with more rigorous metrics (e.g., recall@k with labelled sets)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-ollama-haystack)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
