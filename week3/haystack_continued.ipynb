{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Haystack Continued\n",
    "\n",
    "This notebook is designed for **VS Code** and uses **Ollama** to run local LLM models.\n",
    "\n",
    "**What you’ll do**\n",
    "- Get an Opensearch instance running locally with Podman\n",
    "- Build a RAG Pipeline with Opensearch \n",
    "- Build Streamlit front-end and connect to Pipeline for queries\n",
    "- Explore Hybrid Retrieval with Opensearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d889",
   "metadata": {},
   "source": [
    "### 1. Installing Podman\n",
    "\n",
    "**For MAC OS**\n",
    "```bash\n",
    "#For MAC OS run this in a terminal window\n",
    "brew install --cask podman-desktop #For the GUI application\n",
    "#or\n",
    "brew install podman #For the terminal version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550c6a4",
   "metadata": {},
   "source": [
    "**For Windows:** Download Using the Following link: https://podman-desktop.io/downloads/windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7820b",
   "metadata": {},
   "source": [
    "## 2. Starting up an Opensearch Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3185cc6",
   "metadata": {},
   "source": [
    "Run the following commands in the terminal:\n",
    "```bash\n",
    "#Start up the Podman Machine for the First Time\n",
    "podman machine init\n",
    "podman machine set --rootful #allows port binding without restrictions inside the VM\n",
    "podman machine start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db5887",
   "metadata": {},
   "source": [
    "Run the following command to start up your Opensearch Cluster\n",
    "```bash\n",
    "podman run \\\n",
    "  -p 9200:9200 \\\n",
    "  -p 9600:9600 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"OPENSEARCH_INITIAL_ADMIN_PASSWORD=OSPassword246\" \\\n",
    "  --name opensearch \\\n",
    "  docker.io/opensearchproject/opensearch:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15844",
   "metadata": {},
   "source": [
    "Open a new Terminal window and run the following Curl command to check that your Opensearch Cluster is running and reachable:\n",
    "\n",
    "```bash\n",
    "curl -k -u admin:OSAdmin_123 https://localhost:9200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a RAG Ingestion Pipeline Connected to Opensearch\n",
    "Use some wikepedia pages to ingest as content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can swap out these URLs with any public text URLs you like\n",
    "PUBLIC_URLS = [\n",
    "    \"https://en.wikipedia.org/wiki/Yellow_warbler\",\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://en.wikipedia.org/wiki/Bioluminescence\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a799974",
   "metadata": {},
   "source": [
    "The code below should create the ingestion pipeline and successfully output the required inputs to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fetcher': {'urls': {'type': list[str], 'is_mandatory': True}},\n",
       " 'converter': {'meta': {'type': dict[str, typing.Any] | list[dict[str, typing.Any]] | None,\n",
       "   'is_mandatory': False,\n",
       "   'default_value': None},\n",
       "  'extraction_kwargs': {'type': dict[str, typing.Any] | None,\n",
       "   'is_mandatory': False,\n",
       "   'default_value': None}},\n",
       " 'writer': {'policy': {'type': haystack.document_stores.types.policy.DuplicatePolicy | None,\n",
       "   'is_mandatory': False,\n",
       "   'default_value': None}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to use LinkContentFetcher and HTMLToDocument Components to fetch and convert the texts into Haystack Documents\n",
    "from haystack import Pipeline\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "\n",
    "#initialise all the components here:\n",
    "# --- OpenSearch DocumentStore (local) ---\n",
    "document_store = OpenSearchDocumentStore(\n",
    "    hosts=\"http://localhost:9200\",\n",
    "    index=\"public_texts\",\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    http_auth=(\"admin\", \"OSPassword246\"),\n",
    ")\n",
    "fetcher = LinkContentFetcher( user_agents=[\"ai-mutual-mentorship/0.1 (https://github.com/larry6point6/ai-mutual-mentorship-scheme)\"]) # takes input of URL lists and outputs stream (a list of Bytestream objects)\n",
    "# https://docs.haystack.deepset.ai/docs/linkcontentfetcher\n",
    "converter = HTMLToDocument() # takes a list of Bytestream objects and outputs a list of Haystack Documents\n",
    "# https://docs.haystack.deepset.ai/docs/htmltodocument\n",
    "splitter = DocumentSplitter(split_by=\"word\", split_length=200, split_overlap=15) # takes a list of Haystack Documents and splits them into smaller chunks\n",
    "#https://docs.haystack.deepset.ai/docs/documentsplitter\n",
    "cleaner = DocumentCleaner() # removes emtpy white lines extra spaces and repeated substrings by default (you can add custom cleaning parameters like remove_regex)\n",
    "# https://docs.haystack.deepset.ai/docs/documentcleaner\n",
    "writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "\n",
    "# initalise the ingestion pipeline\n",
    "ingestion_pipeline = Pipeline() \n",
    "\n",
    "# Add all the components to the pipeline\n",
    "ingestion_pipeline.add_component(instance=fetcher, name=\"fetcher\")\n",
    "ingestion_pipeline.add_component(instance=converter, name=\"converter\")\n",
    "ingestion_pipeline.add_component(instance=cleaner, name=\"cleaner\")\n",
    "ingestion_pipeline.add_component(instance=splitter, name=\"splitter\")\n",
    "ingestion_pipeline.add_component(instance=writer, name=\"writer\")\n",
    "\n",
    "# Connect the inputs and outputs of the components together\n",
    "ingestion_pipeline.connect(\"fetcher.streams\", \"converter\") # When there is only one correct type of input/output these can be inferred\n",
    "ingestion_pipeline.connect(\"converter\", \"cleaner\")\n",
    "ingestion_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "ingestion_pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "## Print out the list of required inputs using the following command\n",
    "ingestion_pipeline.inputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d4bdd",
   "metadata": {},
   "source": [
    "Run the ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0221dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion result: {'writer': {'documents_written': 0}}\n",
      "Documents in store: 137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Run the ingestion pipeline ---\n",
    "# Run ingestion\n",
    "result = ingestion_pipeline.run({\"fetcher\": {\"urls\": PUBLIC_URLS}})\n",
    "\n",
    "print(\"Ingestion result:\", result)\n",
    "print(\"Documents in store:\", document_store.count_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 — Vector DBs & OpenSearch (Docker) — Hybrid Retrieval\n",
    "We’ll now use **OpenSearch** as the document store, then compare **BM25**, **dense embeddings**, and the **OpenSearchHybridRetriever**.\n",
    "\n",
    "> Quickstart (local dev):\n",
    "```bash\n",
    "# Single node, security disabled for local testing (see official docs for options)\n",
    "docker run -p 9200:9200 -p 9600:9600   -e \"discovery.type=single-node\"   -e \"DISABLE_SECURITY_PLUGIN=true\"   --name opensearch   -d opensearchproject/opensearch:latest\n",
    "```\n",
    "OpenSearch Haystack integration: `pip install opensearch-haystack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54e786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'splitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Embed with Ollama and write\u001b[39;00m\n\u001b[32m     22\u001b[39m op_embedder = OllamaDocumentEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m docs = \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplitter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m     24\u001b[39m docs_emb = op_embedder.run(docs)\n\u001b[32m     25\u001b[39m doc_store.write_documents(docs_emb[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# index\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'splitter'"
     ]
    }
   ],
   "source": [
    "\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack_integrations.components.retrievers.opensearch import (\n",
    "    OpenSearchBM25Retriever,\n",
    "    OpenSearchEmbeddingRetriever,\n",
    "    OpenSearchHybridRetriever,\n",
    ")\n",
    "\n",
    "# Adjust embedding_dim to your embedding model; nomic-embed-text -> 768\n",
    "OPENSEARCH = {\n",
    "    \"hosts\": [\"http://localhost:9200\"],\n",
    "    \"index\": \"demo_docs\",\n",
    "    \"embedding_dim\": 768,\n",
    "}\n",
    "\n",
    "doc_store = OpenSearchDocumentStore(**OPENSEARCH)\n",
    "\n",
    "# Embed with Ollama and write\n",
    "op_embedder = OllamaDocumentEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT)\n",
    "docs_emb = op_embedder.run(DOCS)\n",
    "doc_store.write_documents(docs_emb[\"documents\"])  # index\n",
    "print(\"OpenSearch indexed docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Three retrievers\n",
    "os_bm25 = OpenSearchBM25Retriever(document_store=doc_store, top_k=5)\n",
    "os_emb  = OpenSearchEmbeddingRetriever(document_store=doc_store, top_k=5)\n",
    "# Hybrid retriever combines both under the hood\n",
    "os_hybrid = OpenSearchHybridRetriever(document_store=doc_store,\n",
    "                                     embedder=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT),\n",
    "                                     top_k=5)\n",
    "\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "print(\"BM25:\")\n",
    "print([d.meta.get(\"source\") for d in os_bm25.run(query=query)[\"documents\"]])\n",
    "print(\"Embedding:\")\n",
    "print([d.meta.get(\"source\") for d in os_emb.run(query_embedding=OllamaTextEmbedder(model=EMBED_MODEL, url=OLLAMA_ENDPOINT).run(text=query)[\"embedding\"])][\"documents\"])  # noqa\n",
    "print(\"Hybrid:\")\n",
    "print([d.meta.get(\"source\") for d in os_hybrid.run(query=query)[\"documents\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-mutual-mentorship-scheme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
