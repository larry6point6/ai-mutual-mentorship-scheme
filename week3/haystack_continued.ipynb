{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Haystack Continued\n",
    "\n",
    "This notebook is designed for **VS Code** and uses **Ollama** to run local LLM models.\n",
    "\n",
    "**What youâ€™ll do**\n",
    "- Get an Opensearch instance running locally with Podman\n",
    "- Build a RAG Pipeline with Opensearch \n",
    "- Build Streamlit front-end and connect to Pipeline for queries\n",
    "- Explore Hybrid Retrieval with Opensearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483045e0",
   "metadata": {},
   "source": [
    "Make sure to to run your Ollama models in a terminal window\n",
    "```bash\n",
    "ollama start\n",
    "ollama pull llama3.2\n",
    "ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d889",
   "metadata": {},
   "source": [
    "## 1. Installing Podman\n",
    "\n",
    "**For MAC OS**\n",
    "```bash\n",
    "#For MAC OS run this in a terminal window\n",
    "brew install --cask podman-desktop #For the GUI application\n",
    "#or\n",
    "brew install podman #For the terminal version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550c6a4",
   "metadata": {},
   "source": [
    "**For Windows:** Download Using the Following link: https://podman-desktop.io/downloads/windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7820b",
   "metadata": {},
   "source": [
    "## 2. Starting up an Opensearch Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3185cc6",
   "metadata": {},
   "source": [
    "Run the following commands in the terminal:\n",
    "```bash\n",
    "#Start up the Podman Machine for the First Time\n",
    "podman machine init\n",
    "podman machine set --rootful #allows port binding without restrictions inside the VM\n",
    "podman machine start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db5887",
   "metadata": {},
   "source": [
    "Run the following command to start up your Opensearch Cluster\n",
    "\n",
    "```bash\n",
    "  podman run \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9600:9600 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"OPENSEARCH_INITIAL_ADMIN_PASSWORD=OSPassword246\" \\\n",
    "    --name opensearch \\\n",
    "    docker.io/opensearchproject/opensearch:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15844",
   "metadata": {},
   "source": [
    "Open a new Terminal window and run the following Curl command to check that your Opensearch Cluster is running and reachable:\n",
    "\n",
    "```bash\n",
    "curl -k -u admin:OSPassword246 https://localhost:9200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a RAG Ingestion Pipeline Connected to Opensearch\n",
    "Use some wikepedia pages to ingest as content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can swap out these URLs with any public text URLs you like\n",
    "PUBLIC_URLS = [\n",
    "    \"https://en.wikipedia.org/wiki/Yellow_warbler\",\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://en.wikipedia.org/wiki/Bioluminescence\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a799974",
   "metadata": {},
   "source": [
    "The code below should create the ingestion pipeline and successfully output the required inputs to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use LinkContentFetcher and HTMLToDocument Components to fetch and convert the texts into Haystack Documents\n",
    "from haystack import Pipeline\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "\n",
    "#initialise all the components here:\n",
    "# --- OpenSearch DocumentStore (local) ---\n",
    "document_store = OpenSearchDocumentStore(\n",
    "    hosts=\"http://localhost:9200\",\n",
    "    index=\"public_texts\",\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    http_auth=(\"admin\", \"OSPassword246\"),\n",
    ")\n",
    "fetcher = LinkContentFetcher( user_agents=[\"ai-mutual-mentorship/0.1 (https://github.com/larry6point6/ai-mutual-mentorship-scheme)\"]) # takes input of URL lists and outputs stream (a list of Bytestream objects)\n",
    "# https://docs.haystack.deepset.ai/docs/linkcontentfetcher\n",
    "converter = HTMLToDocument() # takes a list of Bytestream objects and outputs a list of Haystack Documents\n",
    "# https://docs.haystack.deepset.ai/docs/htmltodocument\n",
    "splitter = DocumentSplitter(split_by=\"word\", split_length=200, split_overlap=15) # takes a list of Haystack Documents and splits them into smaller chunks\n",
    "#https://docs.haystack.deepset.ai/docs/documentsplitter\n",
    "cleaner = DocumentCleaner() # removes emtpy white lines extra spaces and repeated substrings by default (you can add custom cleaning parameters like remove_regex)\n",
    "# https://docs.haystack.deepset.ai/docs/documentcleaner\n",
    "writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "\n",
    "# initalise the ingestion pipeline\n",
    "ingestion_pipeline = Pipeline() \n",
    "\n",
    "# Add all the components to the pipeline\n",
    "ingestion_pipeline.add_component(instance=fetcher, name=\"fetcher\")\n",
    "ingestion_pipeline.add_component(instance=converter, name=\"converter\")\n",
    "ingestion_pipeline.add_component(instance=cleaner, name=\"cleaner\")\n",
    "ingestion_pipeline.add_component(instance=splitter, name=\"splitter\")\n",
    "ingestion_pipeline.add_component(instance=writer, name=\"writer\")\n",
    "\n",
    "# Connect the inputs and outputs of the components together\n",
    "ingestion_pipeline.connect(\"fetcher.streams\", \"converter\") # When there is only one correct type of input/output these can be inferred\n",
    "ingestion_pipeline.connect(\"converter\", \"cleaner\")\n",
    "ingestion_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "ingestion_pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "## Print out the list of required inputs using the following command\n",
    "ingestion_pipeline.inputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d4bdd",
   "metadata": {},
   "source": [
    "Run the ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0221dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run the ingestion pipeline ---\n",
    "# Run ingestion\n",
    "result = ingestion_pipeline.run({\"fetcher\": {\"urls\": PUBLIC_URLS}})\n",
    "\n",
    "print(\"Ingestion result:\", result)\n",
    "print(\"Documents in store:\", document_store.count_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the RAG Retrieval Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a424597",
   "metadata": {},
   "source": [
    "We will now create the second part of the pipeline to run every time a user inputs a query. We will just use the Dense/Embedding retriever for this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4de37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"What is bioluminescence?\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}?\n",
    "\"\"\"\n",
    "\n",
    "# See the prompt builder component here to learn more about prompt templates: https://docs.haystack.deepset.ai/docs/promptbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.document_stores.opensearch import OpenSearchDocumentStore\n",
    "from haystack_integrations.components.retrievers.opensearch import (\n",
    "    OpenSearchEmbeddingRetriever,\n",
    ")\n",
    "# Import OllamaTextEmbedder to embed the user query\n",
    "from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder\n",
    "# Import the OllamaGenerator to generate the response to the user query with the retrieved documents as context\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "# Import PromptBuilder to construct the new prompt with context for the generator\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "# Re-initialise the Opensearch document store as was done previously\n",
    "document_store = OpenSearchDocumentStore(\n",
    "    hosts=\"http://localhost:9200\",\n",
    "    index=\"public_texts\",\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    http_auth=(\"admin\", \"OSPassword246\"),\n",
    ")\n",
    "\n",
    "# Initialise the embedding and generation models to embed the user query\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "GENERATION_MODEL = \"llama3.2\"\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434\"\n",
    "TOP_K = 5\n",
    "\n",
    "query_embedder = OllamaTextEmbedder(\n",
    "    model=EMBED_MODEL,\n",
    "    url=OLLAMA_ENDPOINT,\n",
    ")\n",
    "\n",
    "# Initialise the Ollama text generator component\n",
    "response_generator = OllamaGenerator(\n",
    "    model=GENERATION_MODEL,\n",
    "    url=OLLAMA_ENDPOINT,\n",
    ")\n",
    "\n",
    "# Initialise the embedding retriever \n",
    "embedding_retriever = OpenSearchEmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    top_k=TOP_K,\n",
    ")\n",
    "\n",
    "\n",
    "#Initialise the prompt builder\n",
    "prompt_builder = PromptBuilder(\n",
    "    template=prompt_template,\n",
    "    required_variables=[\"query\", \"documents\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG Pipeline and connect the components for the dense retrieval approach (query embedding + OpenSearch embedding retriever)\n",
    "retrieval_pipeline = Pipeline()\n",
    "retrieval_pipeline.add_component(\"query_embedder\", query_embedder)\n",
    "retrieval_pipeline.add_component(\"retriever\", embedding_retriever)\n",
    "retrieval_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "retrieval_pipeline.add_component(\"response_generator\", response_generator)\n",
    "retrieval_pipeline.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "retrieval_pipeline.connect(\"retriever\", \"prompt_builder\")\n",
    "retrieval_pipeline.connect(\"prompt_builder\", \"response_generator\")\n",
    "\n",
    "# Print out the list of required inputs for the retrieval pipeline\n",
    "retrieval_pipeline.inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8e63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the retrieval pipeline with the user query as input\n",
    "result = retrieval_pipeline.run({\"query_embedder\": {\"text\": query}, \"prompt_builder\": {\"query\": query}},\n",
    "                                include_outputs_from=\"retriever\") # we need to give the query to both the query embedder and prompt builder\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14710168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print just the response and retrieved documents\n",
    "print(\"Generated response:\", result[\"response_generator\"][\"replies\"][0])\n",
    "\n",
    "print(\"Retrieved documents:\")\n",
    "for doc in result[\"retriever\"][\"documents\"]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661ba97",
   "metadata": {},
   "source": [
    "## 5. Connecting a simple front-end\n",
    "\n",
    "We will now export all of the pipeline code into retriever.py. Then run the streamlit app in app.py to run queries through our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9957c02",
   "metadata": {},
   "source": [
    "## Bonus/Take Home Task\n",
    "\n",
    "See if you can change the retrieval from Dense Embedding retrieval to Sparse BM25 or even Hybrid Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430618b3",
   "metadata": {},
   "source": [
    "HINT: The BM25 retriever will not need a query embedding and the hybrid retrieval works slightly differently as it's own pipeline. See the docs for both below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f087f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-mutual-mentorship-scheme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
